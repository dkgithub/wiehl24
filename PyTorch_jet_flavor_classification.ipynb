{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkgithub/wiehl24/blob/main/PyTorch_jet_flavor_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IduQrGpXTAlb"
      },
      "source": [
        "# Hands-on exercise for \"Mastering Model Building\"\n",
        "(inspired heavily by previous edition by M.Rieger but\n",
        " adapted to PyTorch and skorch)\n",
        "\n",
        "Before we start with the actual tutorial, the notebook needs to be quickly set up. For this, execute the next cell which installs some dependencies that are not deployed by default on colab. This should only take a couple seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UryureMsrZAv"
      },
      "outputs": [],
      "source": [
        "#!rm -rf helpers\n",
        "![ ! -d helpers ] && git clone --recursive https://github.com/dkgithub/erum_ml_school_helpers helpers\n",
        "!pip install wget torchinfo skorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w86zqibfTAlf"
      },
      "source": [
        "To verify that the setup worked, run the next cell which imports the `helpers` package that we will use below. The tutors will gladly help you in case you spot an issue 😀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odtxXlF_B7yz"
      },
      "outputs": [],
      "source": [
        "# load the helpers package and other software\n",
        "import helpers as hlp\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGJri01XvQfK"
      },
      "outputs": [],
      "source": [
        "#delete\n",
        "from helpers import specs\n",
        "specs.data_dir = '/beegfs/desy/user/kruecker/work/data/JetTagSchool/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyuDxu0gTAlg"
      },
      "source": [
        "No issues? Great 🎉!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJJXdDOLTAlg"
      },
      "source": [
        "# Jet classification using PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyYC8ORgTAlg"
      },
      "source": [
        "<br />\n",
        "<img src=\"https://github.com/dkgithub/wiehl24/blob/main/img/top.png?raw=true\" width=\"500\" >\n",
        "\n",
        "(image: https://www.particlezoo.net/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsjS-lWTAlg"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btlHuo-oTAlg"
      },
      "source": [
        "Our task will be to discriminate between jets produced by a hadronically decaying top quark (**top jet**), or jets produced by a light flavour quark or a gluon (**light / QCD jet**).\n",
        "Both hadronize into a large number of particles:\n",
        "\n",
        "<center><img src=\"https://github.com/riga/qu_tf_tutorial/blob/master/assets/top_vs_qcd.png?raw=1\" width=\"60%\"/></center>\n",
        "\n",
        "- Quark/gluon jets fragment into hadrons\n",
        "- Top quark decays first into three *sub jets* and fragment then into hadrons\n",
        "- But: at high momentum they may be merged\n",
        "\n",
        "→ Substructure might be resolvable by a NN and serve as a handle for classification!\n",
        "\n",
        "<img src=\"https://github.com/dkgithub/wiehl24/blob/main/img/boosted_top_decay.png?raw=1\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JZb1WB_TAlh"
      },
      "source": [
        "## Aim and scope of the tutorial\n",
        "\n",
        "You are given a dataset with information on > 1M jets, containing **kinematic observables** as well a **truth** label (a float value, either 0 or 1) that describes the origin of the jet. You can find more information on the input data in the cells below.\n",
        "\n",
        "**We will create a neural (NN) network model that, given a jet, uses its kinematic observables as input features to predict its most probable origin (\"class\" in ML terms)!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTQFKXbTAlh"
      },
      "source": [
        "> The **main goal** is not to write a perfectly working model that is tailored for one specific use case, but rather to get hands-on experience with PyTorch and some of the fundamental concepts used in machine learning today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD4MlkHrTAlh"
      },
      "source": [
        "## Outline\n",
        "\n",
        "- Part 1: PyTorch introduction\n",
        "- Part 2: Refresher of NN terminology\n",
        "- Part 3: The tutorial dataset\n",
        "- Part 4: Minimal training and evaluation workflow\n",
        "- Part 5: Advanced training with skorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDJjW-cUTAlh"
      },
      "source": [
        "# Part 1: PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_V6A5WKTAli"
      },
      "source": [
        "## PyTorch ingredients\n",
        "\n",
        "1. NumPy-like array structure with torch **tensors**. See torch_tensor_tutorial_basics.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SFG6QTg2Gea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK0Jxm_zTAli"
      },
      "outputs": [],
      "source": [
        "t = torch.arange(5)\n",
        "print(t)\n",
        "\n",
        "# all elements * 2\n",
        "print(t * 2)\n",
        "\n",
        "# broadcasting as in numpy\n",
        "t=t[:,None] # None as index to extend number of dimension\n",
        "print(t.shape,t.T.shape)\n",
        "A=t-t.T # this is a matrix\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLUCFN9sTAli"
      },
      "source": [
        "## PyTorch ingredients\n",
        "\n",
        "2. Built-in operation for neural network building in torch.nn([docs](https://pytorch.org/docs/stable/nn.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSZ3d2zxDDdd"
      },
      "outputs": [],
      "source": [
        "# torch provides random numbers\n",
        "t = torch.randn((1,4,2))\n",
        "print(t.shape)\n",
        "print(t)\n",
        "\n",
        "# multiple activation functions\n",
        "print(torch.relu(t))\n",
        "print(torch.sigmoid(t))\n",
        "# There are multiple namespaces where activation functions appear.\n",
        "# By historic reasons ...\n",
        "print(torch.nn.functional.relu(t))\n",
        "# But different nn.ReLU() <- this is part of a neural net\n",
        "aRelu = torch.nn.ReLU()\n",
        "print(aRelu(t))\n",
        "\n",
        "# softmax along dim=2\n",
        "torch.softmax(t,dim=2)\n",
        "\n",
        "# a linear layer\n",
        "layer=torch.nn.Linear(2,2,bias=True)\n",
        "layer(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB-MQwk3HZaD"
      },
      "outputs": [],
      "source": [
        "# torch numpy bridge\n",
        "# from_numpy\n",
        "np_array    = np.array([[1,2],[3,4]])\n",
        "torch_tensor = torch.from_numpy(np_array)\n",
        "print( type(torch_tensor), '\\n', torch_tensor)\n",
        "\n",
        "# and back\n",
        "# .numpy()\n",
        "print( type(torch_tensor.numpy()), '\\n', torch_tensor.numpy())\n",
        "\n",
        "# if you just need a standard Python number\n",
        "print(torch_tensor[0,0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AoCxeXUTAli"
      },
      "source": [
        "## PyTorch ingredients\n",
        "\n",
        "3. Most important **automatic differentiation** on regular python code. See autograd tutorial\n",
        " (torch_autograd_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWZxEwRlTAlj"
      },
      "source": [
        "## PyTorch ingredients\n",
        "\n",
        "Easy placement of tensors and models to computing device, e.g. GPU, MPS (Apple M1 etc.)\n",
        "\n",
        "```python\n",
        "#(this is not executable)\n",
        "if torch.cuda.is_available():\n",
        " dev = \"cuda:0\"\n",
        "else:\n",
        " dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "\n",
        "# place a tensor\n",
        "a = torch.zeros(4,3)\n",
        "a = a.to(device)\n",
        "\n",
        "# place a network\n",
        "model = model.to(device)\n",
        "\n",
        "```\n",
        "Make sure that you use a runtime with GPU (T4 in Colab). If there is none available, the notebooks works as well with cpu but slower. I have observe a factor of 4 speed up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXXk9HjBUQRL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('torch',torch.__version__)\n",
        "print('device type is',device)\n",
        "if device == 'cuda' :print(torch.cuda.get_device_name())\n",
        "from os import environ\n",
        "if \"COLAB_TPU_ADDR\" in environ and environ[\"COLAB_TPU_ADDR\"]:\n",
        "  print(\"A TPU is connected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWgpNvX7TAlj"
      },
      "source": [
        "# Part 2: NN terminology refresher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcBtgu8PTAlk"
      },
      "source": [
        "Before we dive in, let's quickly introduce a minimal set of NN terminology. This is a recap of what had been discussed in the lecture. Please note that a full introduction into machine learning would be too exhaustive at this point. For more insights, checkout one of the many resources that are publically available.\n",
        "\n",
        "One of many examples of a well written introduction that even includes code examples and interactive visualizations is the free online book at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)\n",
        " (see lecture for more references)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POVINVibTAlk"
      },
      "source": [
        "<center><img src=\"https://github.com/riga/qu_tf_tutorial/blob/master/assets/nn_graph.png?raw=1\" width=\"60%\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJXgZfrTAlk"
      },
      "source": [
        "- NNs are structured into so-called **layers** that contain **units**.\n",
        "- We denote the **input layer** with $l = 0$, and the **output layer** with $l = L$.\n",
        "- When the number of input features we give to the network is $N$, then the input layer will have $N$ units.\n",
        "- The grey circles in the graph denote **bias units**. Their value is always one to allow for some numeric independence (see below).\n",
        "- These $N$ input values are **forward passed** to layer $l = 1$. We do not intend to directly feed values into it, nor do we manually extract its output values - hence we call it **hidden layer**.\n",
        "- The forward pass involves **weights** - these are free parameters, which are updated during training, and can be understood as *fit parameters*. NNs with millions of parameters are common!\n",
        "- The input to the *i-th* unit in layer $l = 1$, $z_i^l$, is simply the scalar product of the vector of input features and a vector of weights $W_{ij}^{l-1}$,\n",
        "\\begin{align}\n",
        "z_i^l &= \\sum_j W_{ij}^{l-1} \\cdot a_j^{l-1} + b_i^{l-1}\\\\\n",
        "\\Rightarrow z^l &= W^{l-1} \\cdot a^{l-1} + b^{l-1}\n",
        "\\end{align}\n",
        "where the bias unit is considered as an additional input with value 1, multiplied by the weight $b_j^{l-1}$. Each particular value of $W_{ij}^{l-1}$ and $b_j^{l-1}$ is represented by arrows in the diagram. The second line uses a **vectorized formulation** over all $1 \\leq j \\leq N$ inputs.\n",
        "- The output $a_i^l$ of this unit is the value of an [**activation** function](https://en.wikipedia.org/wiki/Activation_function) $\\sigma$, evaluated at its input $z_i^l$. This reads\n",
        "$$\n",
        "\\begin{align}\n",
        "a^l = \\sigma^l(z^l) = \\sigma_l(W^{l-1} \\cdot a^{l-1} + b^{l-1})\n",
        "\\end{align}\n",
        "$$\n",
        "in the vectorial form. The same function is applied to all units in the same layer.\n",
        "- Just like what we did with the input features, these output values can be propagated through the entire network up to the output layer $l = L$. Here, we choose a clever activation function to output the response $y$ that we like the network to learn (more on this later).\n",
        "- As a whole, we can see the network output $y$ as a function of $x$, given $W$ and $b$. Thus, we can write\n",
        "$$\n",
        "\\begin{align}\n",
        "y = y(x | \\underbrace{W,b}_{\\equiv\\ \\omega}) = \\underbrace{(a^L \\circ a^{L-1} \\circ \\dots \\circ a^1)_\\omega}_\\text{model} (x)\n",
        "\\end{align}\n",
        "$$\n",
        "and identify the **model** as the concatenation of all layers given the free parameters $\\omega$.\n",
        "- Besides the clear mathematical construction and rules to perform the forward pass, you might notice some *room for choices*, such as the number of hidden layers, the amount of units per layer, or the activation functions. These are called **hyper-parameters** and it is your task to understand your input data as well as the problem you want to solve, and adjust these parameters in a educated fashion to optimize the network performance.\n",
        "- It is this exploration of the huge **space** of hyper-parameters what makes working with NNs complex and exciting at the same time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekTsji24TAlk"
      },
      "source": [
        "## Training, overtraining and data splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blvgBIl-TAll"
      },
      "source": [
        "## Network training\n",
        "\n",
        "A comprehensive desription and explanantion of the *art* of NN training requires a collection of concepts and mathematical proofs that would by far reach beyond the scope of this tutorial. However, if you intend to apply machine learning techniques in the future, it is **highly recommended** to dive into this fascinating topic. Especially the formulation and proof of the **backpropagation** algorithm is of major importance as it paved the way of actual **deep** neural networks! A good place to start is the free online book at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0rb6qXyTAlm"
      },
      "source": [
        "## Overtraining\n",
        "\n",
        "During training, a NN receives *examples* in the form of vectors of input features and produces a prediction which can be compared to the *true* value one would expect. Based on the difference between prediction and expectation (in whatever way *difference* is defined), the network receives an either good or bad feedback that is used to update its trainable parameters, ideally leading to an improvement in the next iteration.\n",
        "\n",
        "However, this process can have some caveats! When the amount of available training examples is very limited, chances are that they might not describe underlying probability distributions with sufficient precision. As a result, the NN might start to develop a bias towards this particular set of examples. Then, after the training phase, when the NN is requested to evaluate examples that it never *saw* before, its ability to infer predictions might differ greatly from what you observed during training. The model fails to **generalize**, which is referred to as **overtraining**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wQk79jfTAln"
      },
      "source": [
        "## Network capacity\n",
        "\n",
        "<center><img src=\"https://github.com/riga/qu_tf_tutorial/blob/master/assets/nn_capacity.png?raw=1\" width=\"60%\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EnozDFcTAln"
      },
      "source": [
        "## Data splitting\n",
        "\n",
        "There are several techniques to monitor and prevent overtraining. One **mandatory** monitoring technique is data splitting. In its easiest form, the entirety of examples is split into three datasets:\n",
        "\n",
        "- The *training* dataset is used for the actual training procedure.\n",
        "- The *validation* dataset is **not** used **for** the training itself, but **during** the training to immediately monitor the NN's ability to generalize.\n",
        "- All actual measurements are performed on an independent *testing* dataset.\n",
        "\n",
        "This splitting is applied throughout this exercise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9l2d_2TAln"
      },
      "source": [
        "# Part 3: Tutorial dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yhETp2YTAln"
      },
      "source": [
        "The input data for this tutorial consists of 1 million jets, originating from either\n",
        "  - hadronically decaying top quarks (our **signal** ✔︎), or\n",
        "  - dijet QCD events (our **background** ✘),\n",
        "\n",
        "and clustered using the $k_{T}$ algorithm with $\\Delta R$ = 0.8.\n",
        "\n",
        "<br />\n",
        "\n",
        "Data was generated using Phythia & Delphes, configured\n",
        "  - to collide protons at 14 TeV center-of-mass energy,\n",
        "  - to generate jets with a $p_{T}$ range of [550, 650] GeV (before hadronization ❗️), and\n",
        "  - **without** mixing in pileup events for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi3NvZjQTAlo"
      },
      "source": [
        "### Input features\n",
        "\n",
        "Per jet, you are given the four-vectors of up to **200** of its *constituents* (i.e., the particles that form the jet by means of clustering).\n",
        "\n",
        "   - These up to 800 values define your **input features**.\n",
        "   - Note that not all jets have that many constituents❗️\n",
        "   - To spare you the trouble of working with uneven (so-called *jagged*) arrays, these \"missing\" constituents vectors are filled with zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H2AFUFNTAlo"
      },
      "source": [
        "### Training targets\n",
        "\n",
        "Per jet, you are provided 2 different training targets:\n",
        "\n",
        "  1. A flag that marks the true origin of the jet\n",
        "    - `1` for jets from top quark decays\n",
        "    - `0` for light jets from QCD events\n",
        "  2. The true four-vector of the initial particle (only for top quarks)\n",
        "\n",
        "For now, we will focus on the **1.** training target to perform a *classification* to answer the question: **Top or not?**\n",
        "\n",
        "(Spoiler: the second target is used later on to include an energy regression 🤫)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5LfKWrjTAlo"
      },
      "source": [
        "## Diving into the data\n",
        "\n",
        "Let's check out the data! It is stored in NumPy arrays across several files, with 50k jets per file. This way, prototyping and test runs are way quicker. You are given\n",
        "\n",
        "- 20 training files (`\"train\"`)\n",
        "- 8 validation files (`\"valid\"`)\n",
        "- 8 testing files (`\"test\"`)\n",
        "\n",
        "A few tools to perform recurrent tasks such as data loading are available in the dedicated `helpers` package. Let's load two training files and inspect the contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXRqDckUTAlo"
      },
      "outputs": [],
      "source": [
        "# load the content of two \"train\" files - may take a few minutes on first loading\n",
        "# these are numpy arrays\n",
        "c_vectors, true_vectors, labels = hlp.data.load(\"train\", start_file=0, stop_file=2)\n",
        "c_vectors.shape, true_vectors.shape, labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjGagrkyTAlp"
      },
      "source": [
        "All arrays have 100k (2 files x 50k jets) *rows* (dimension 0).\n",
        "\n",
        "- Per jet, we have up to 200 constituents (`c_vectors`) with 4 variables ($E$, $p_x$, $p_y$, $p_z$) each, thus `(200, 4)`.\n",
        "- The initiating quark `true_vectors` only has 4 values per jet = one 4-vector.\n",
        "- The `labels`, however, are single values: 0 or 1.\n",
        "\n",
        "Let's create a few plots to get some insights into our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14v5yb7aTAlp"
      },
      "outputs": [],
      "source": [
        "# define some flags to make four-vector element access more verbose\n",
        "E, PX, PY, PZ = range(4)\n",
        "jet_names = (\"Top jet\", \"QCD jets\")\n",
        "is_top = labels.reshape(-1) == 1\n",
        "is_qcd = labels.reshape(-1) != 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEMuBuDiTAlp"
      },
      "source": [
        "## Truth distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf4aERjOTAlp"
      },
      "outputs": [],
      "source": [
        "# distribution of truth labels\n",
        "hlp.plots.plot_hist(\n",
        "    (labels[is_top], labels[is_qcd]),\n",
        "    range=(0, 1),\n",
        "    names=jet_names,\n",
        "    xlabel=\"Label distribution\",\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT6tHBM78q87"
      },
      "source": [
        "We have equal number of events per class. That makes the training easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU_rjyQOTAlp"
      },
      "outputs": [],
      "source": [
        "# energy distribution of the true top quark particle\n",
        "# remember, this is only available for top jets (zero otherwise)\n",
        "hlp.plots.plot_hist(\n",
        "    true_vectors[is_top,E],\n",
        "    names=(\"Top jets\",),\n",
        "    xlabel=\"True energy / GeV\",\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lozNhMi-Q976"
      },
      "outputs": [],
      "source": [
        "# energy distribution of the true top quark particle with a log scale\n",
        "hlp.plots.plot_hist(\n",
        "    true_vectors[is_top, E],\n",
        "    names=(\"Top jets\",),\n",
        "    xlabel=\"True energy / GeV\",\n",
        "    log=True\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDpDDmJ-TAlp"
      },
      "outputs": [],
      "source": [
        "# px distribution of the true particle\n",
        "hlp.plots.plot_hist(\n",
        "    true_vectors[is_top, PX],\n",
        "    names=(\"Top jets\",),\n",
        "    xlabel=\"True $p_x$ / GeV\",\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHNwp_bBTAlq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# mass distribution of the true particle\n",
        "mass = (true_vectors[:, E]**2 - np.sum(true_vectors[:, PX:]**2, axis=1))**0.5\n",
        "hlp.plots.plot_hist(\n",
        "    mass[is_top],\n",
        "    names=(\"Top jets\",),\n",
        "    xlabel=\"True mass / GeV\",\n",
        "    log=False,\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhE4I9xnTAlq"
      },
      "source": [
        "This is actually a top quark! (it should have a mass of ~172.76 GeV). Unfortunately, we don't have this information about the truth in real data. There we have to work with the measured constituents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnOnbb5TAlq"
      },
      "source": [
        "## Input feature distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvsvWrS5TAlq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# energy distribution of all constituents\n",
        "e_c_top = c_vectors[is_top, :, E].flatten()\n",
        "e_c_qcd = c_vectors[is_qcd, :, E].flatten()\n",
        "# store a mask to remove zeros\n",
        "non_zero_top = e_c_top != 0\n",
        "non_zero_qcd = e_c_qcd != 0\n",
        "\n",
        "hlp.plots.plot_hist(\n",
        "    (e_c_top[non_zero_top], e_c_qcd[non_zero_qcd]),\n",
        "    names=jet_names,\n",
        "    log=True,\n",
        "    xlabel=\"Constituents energy / GeV\",\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqVlspVE-qZB"
      },
      "outputs": [],
      "source": [
        "print(f'Minimal energy in top constituenten {e_c_top[non_zero_top].min():3.2e} max {e_c_top.max():3.2e}')\n",
        "print(f'Minimal energy in top constituenten {e_c_qcd[non_zero_qcd].min():3.2e} max {e_c_qcd.max():3.2e}')\n",
        "print(f'We have an dynamic range of more then 5 orders of magnitudes.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTtObz-gTAlq"
      },
      "outputs": [],
      "source": [
        "# px distribution of all constituents, zeros removed with the mask defined above\n",
        "px_c_top = c_vectors[is_top, :, PX].flatten()\n",
        "px_c_qcd = c_vectors[is_qcd, :, PX].flatten()\n",
        "\n",
        "hlp.plots.plot_hist(\n",
        "    (px_c_top[non_zero_top], px_c_qcd[non_zero_qcd]),\n",
        "    names=jet_names,\n",
        "    log=True,\n",
        "    xlabel=\"Constituents $p_x$ / GeV\",\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWZSpc1yTAlq"
      },
      "outputs": [],
      "source": [
        "# pz distribution of all constituents\n",
        "pz_c_top = c_vectors[is_top, :, PZ].flatten()\n",
        "pz_c_qcd = c_vectors[is_qcd, :, PZ].flatten()\n",
        "hlp.plots.plot_hist(\n",
        "    (pz_c_top[non_zero_top], pz_c_qcd[non_zero_qcd]),\n",
        "    names=jet_names,\n",
        "    log=True,\n",
        "    xlabel=\"Constituents $p_z$ / GeV\",\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also calculate the invariant mas of the sum of all jet constituents."
      ],
      "metadata": {
        "id": "YOG44SyK77BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# invariant mass of sum of jet constituents by type\n",
        "def calculateJetMass(c_vecs):\n",
        "  sum=c_vecs.sum(axis=1)\n",
        "  m2=sum[:,E]**2-sum[:,PX]**2-sum[:,PY]**2-sum[:,PZ]**2\n",
        "  return np.sqrt(m2)\n",
        "\n",
        "m_top=calculateJetMass(c_vectors[is_top])\n",
        "m_qcd=calculateJetMass(c_vectors[is_qcd])\n",
        "\n",
        "hlp.plots.plot_hist(\n",
        "    (m_top, m_qcd),\n",
        "    names=jet_names,\n",
        "    log=False,\n",
        "    xlabel=\"jet mass GeV\",\n",
        ").show()"
      ],
      "metadata": {
        "id": "8i3-_OFl72ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest tagger one can think of would be a cut on the invariant mass of all jet constituents. From the plot above a cut of 140 GeV seems to be reasonable. For this, we can calculate the accuracy by counting the correct and false jets:"
      ],
      "metadata": {
        "id": "IaEiSetv8Nb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all = len(m_qcd)+len(m_top)\n",
        "n_top_true  = np.sum(m_top>=140)\n",
        "n_top_false = np.sum(m_qcd>=140)\n",
        "n_qcd_true  = np.sum(m_qcd <140)\n",
        "n_qcd_false = np.sum(m_top <140)\n",
        "accuracy = (n_top_true+n_qcd_true)/all\n",
        "print(f'Accuracy {100*accuracy:.2f} %')"
      ],
      "metadata": {
        "id": "nM8MKx0a8TuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our neural network should at least be better than 89%.\n",
        "\n",
        "Next, we look at the consituents that are really filled with data and not just zero padded."
      ],
      "metadata": {
        "id": "XykPLevO8Z9x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz21ts-ETAlq"
      },
      "outputs": [],
      "source": [
        "# number of constituents per jet\n",
        "# remember, missing constituents are filled with zeros, so we take the energy value as a marker\n",
        "n_c_top = np.count_nonzero(c_vectors[is_top, :, E], axis=1)\n",
        "n_c_qcd = np.count_nonzero(c_vectors[is_qcd, :, E], axis=1)\n",
        "hlp.plots.plot_hist((n_c_top, n_c_qcd), names=jet_names, xlabel=\"N constituents per jet\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm1M-8R7TAlq"
      },
      "source": [
        "## Lessons learned\n",
        "\n",
        "Altough we are repared to handle *up to* 200 constituents per jet, only a few of them seem to have more than 100 constituents!\n",
        "\n",
        "Ok, so now that we understood the data, it would not make sense to include all these zeros in a network training. We can safely pick only the first, say, **120 constituents**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oam-GShgTAlr"
      },
      "source": [
        "# Part 4: Minimal training and evaluation workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg13DaRNdwIQ"
      },
      "source": [
        "In PyTorch the deep learning *mechanics* is easily visible. We must\n",
        "* Construct the **learning loop** explicitly,\n",
        "  * Here, the torch **dataloader** is used\n",
        "* Define an **optimizer**,\n",
        "* Calculate the **loss**,\n",
        "* Calculate and apply the **gradient**\n",
        "* Check the loss development, accuracy etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUvlhZwTAlr"
      },
      "source": [
        "Before we study the possible improvements, we define a base model as starting point. First, we have to decide how to feed the data into the network. We start with just flattening the first 120 4-vectors. This gives as 480 dimensional input vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1b61B7UTAlr"
      },
      "outputs": [],
      "source": [
        "# first, we define a preprocessing function that (e.g.) takes the\n",
        "# constiuents and returns an other representation of them\n",
        "# in this case, we select only the first 120 constituents and\n",
        "# flatten the resulting array from (..., 120, 4) to (..., 480,)\n",
        "# i.e. we just apply a simple feedforward network and all constituents\n",
        "# are treated equally\n",
        "def preprocess_constituents(constituents):\n",
        "    return constituents[:, :120].reshape((-1, 480))\n",
        "# later we try something more fancy\n",
        "#def preprocess_constituents(constituents):\n",
        "#     c_sum=constituents.sum(axis=1)\n",
        "#     metric=np.array([1.,-1.,-1.,-1.])\n",
        "#     c_inv=(constituents*metric*c_sum[:,None,:]).sum(axis=2)\n",
        "#     return c_inv.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gNAaErPD1ds"
      },
      "source": [
        "* For binary classifier we can either use 2 outputs with softmax or, what we will do, 1 output with a sigmoid final node. The (0,1) labels are just appropriate for this.\n",
        "* We could load more data but to observe our models 100k are sufficient for now. The traing will be a bit faster. For best results, we will eventually load all 8 files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q1ChW4TTAlr"
      },
      "outputs": [],
      "source": [
        "# load more training, and also validation data\n",
        "# (increase the numbers if your system allows it)\n",
        "#c_vectors_train, _, labels_train = hlp.data.load(\"train\", stop_file=8)\n",
        "#c_vectors_valid, _, labels_valid = hlp.data.load(\"valid\", stop_file=2)\n",
        "c_vectors_train, _, labels_train = hlp.data.load(\"train\", stop_file=2)\n",
        "# the validation data is not yet loaded. It will take a few minutes on first load\n",
        "c_vectors_valid, _, labels_valid = hlp.data.load(\"valid\", stop_file=1)\n",
        "\n",
        "# run the preprocessing\n",
        "c_vectors_train = preprocess_constituents(c_vectors_train)\n",
        "c_vectors_valid = preprocess_constituents(c_vectors_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc4IrOaPyamV"
      },
      "outputs": [],
      "source": [
        "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "#scaler = StandardScaler()\n",
        "##scaler = MinMaxScaler()  # worst idea\n",
        "#scaler.fit(c_vectors_train)\n",
        "#c_vectors_train=scaler.transform(c_vectors_train)\n",
        "#c_vectors_valid=scaler.transform(c_vectors_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9bjzQgpKyNH"
      },
      "source": [
        "* Our data consists of *numpy arrays*. We first create *torch.tensors* from them.\n",
        "* PyTorch provides a class *dataset*. It is useful to zip together data and target/label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_qupRfbTl_"
      },
      "outputs": [],
      "source": [
        "# We created torch tensors from numpy arrays, map to float32 and move to GPU if available\n",
        "\n",
        "c_tensor_train      = torch.from_numpy(c_vectors_train).float().to(device)\n",
        "label_tensor_train  = torch.from_numpy(labels_train).float().to(device)\n",
        "\n",
        "c_tensor_valid      = torch.from_numpy(c_vectors_valid).float().to(device)\n",
        "label_tensor_valid  = torch.from_numpy(labels_valid).float().to(device)\n",
        "\n",
        "c_vectors_train.shape, label_tensor_train.shape,c_vectors_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5d8VjBea4ur"
      },
      "outputs": [],
      "source": [
        "# Then, we create a dataset from our tensors\n",
        "# dataset: a special torch data structure that can be used within a dataloader\n",
        "\n",
        "dataset_train = torch.utils.data.TensorDataset(c_tensor_train,label_tensor_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfL4ZKdjLHEs"
      },
      "source": [
        "* The PyTorch *dataloader* fulfils  several purposes. Most important for us, it cuts the data into random (*shuffle*) batches of *batch_size*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE24iR8wajnN"
      },
      "outputs": [],
      "source": [
        "# Next, we build the the dataloader\n",
        "\n",
        "batch_size=500\n",
        "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle = True)\n",
        "n_inner=float(len(train_loader))\n",
        "\n",
        "print(f'The dataloader is able to produce {int(n_inner):3d} batches of {batch_size} data points each.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS8NijjETCMp"
      },
      "source": [
        "The easiest way in PyTorch to create a simple network is the *Sequential class*. Complex models are typically defined as a special derived classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28HRhljOTAlr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "import torchinfo # for nice model summary\n",
        "\n",
        "# define the model\n",
        "# - 2 hidden layers\n",
        "# - 128 units each\n",
        "# - tanh activation\n",
        "# - 1 output unit with sigmoid activation\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(480, 256, bias=True),\n",
        "    nn.Tanh(),\n",
        "    #nn.BatchNorm1d(256),\n",
        "    nn.Linear(256, 128, bias=True),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128,   1, bias=True),\n",
        "    nn.Sigmoid(),\n",
        ")\n",
        "\n",
        "\n",
        "# If available, move the model to the GPU - if not, device had been set to 'cpu' above.\n",
        "model = model.to(device)\n",
        "# We use Adam as optimizer and start with learning rate 0.005\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# To make your life easier, the helpers provide a\n",
        "# history object for plotting the learning curves\n",
        "# We track the average loss, the loss on the validation samples,\n",
        "# and the accuracy on the validation sample\n",
        "# (accuracy = fraction of correctly classified events).\n",
        "\n",
        "L = hlp.plots.history(\"loss\",\"val_loss\",\"acc\") # for learning curves\n",
        "torchinfo.summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQrb_1y1PAuO"
      },
      "source": [
        "Our model contains more then 150k trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_SDrqBfakra"
      },
      "source": [
        "Now we are prepared to build the trainings loop. All the elements of the stochastic gradient descent are visible.\n",
        "* At start, all gradients are set to zero\n",
        "* The batch is read in\n",
        "* The network is applied to the batch\n",
        "* The prediction of the network is compared to the truth by the loss\n",
        "* Automatic differentiation is applied to the loss\n",
        "to calculate the gradients of all trainable parameters\n",
        "* The gradients times learning rate etc. is applied by the optimizer to the network to update weights and biases\n",
        "* The process is repeated untill all data is processed (one epoch)\n",
        "* The complete data is reused for a certain number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGbC1aM2NHrr"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "def accuracy(pred_labels,true_labels):\n",
        "    equal = (pred_labels>0.5)==(true_labels>0.5)\n",
        "    return equal.float().mean()\n",
        "\n",
        "# we use binary cross entropy as loss\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "\n",
        "t0 = time() # to measure how much time we need\n",
        "tdelta=0\n",
        "\n",
        "# numer of epochs we will run the training\n",
        "n_epochs=40\n",
        "for ep in range(n_epochs): # <<< the epoch loop\n",
        "\n",
        "    av_loss=0\n",
        "    model.train()\n",
        "    tstart=time()\n",
        "    for c,label in train_loader: # <<< the batch loop\n",
        "\n",
        "        #for debugging if we get lost with the shapes\n",
        "        #print(c.shape,label.shape)\n",
        "        #break\n",
        "\n",
        "        # set all gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # apply the model to one batch of data\n",
        "        y_pred = model(c)\n",
        "        #for debugging if we get lost with the shapes\n",
        "        #print(y_pred.shape,label.shape)\n",
        "        #break\n",
        "\n",
        "        # calculate the loss by comparing prediction and truth\n",
        "        loss = loss_fn(y_pred,label)\n",
        "\n",
        "        # calculate the gradients with respect to all NN weights and biases\n",
        "        loss.backward()\n",
        "\n",
        "        # apply the gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # we calculate the average loss during training\n",
        "        with torch.no_grad():\n",
        "            av_loss+= loss.item()\n",
        "\n",
        "    # before we start the next epoch we have a look at the average loss on training data\n",
        "    av_loss/=n_inner\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        y_pred = model(c_tensor_valid)\n",
        "        loss   = loss_fn(y_pred,label_tensor_valid).item()\n",
        "        # we calculate the accuracy, i.e. the percentage of correct classified events\n",
        "        acc    = accuracy(y_pred, label_tensor_valid).item()\n",
        "        dt=time() - t0\n",
        "        print(f\"Epoch {ep} ({dt:3.2f}sec) av_loss {av_loss:5.4f} validation loss {loss:5.4f} acc {acc*100:6.2f}%\")\n",
        "        L.append(av_loss,loss,acc*100)\n",
        "    # LR cool down\n",
        "    # if ep >10: optimizer.param_groups[0]['lr']*=0.95\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-xIj17AP5yi"
      },
      "outputs": [],
      "source": [
        "# You can run the cell above several times to accumulate more epochs and then plot the history.\n",
        "# If you need a fresh training with a modified model, rerun the above cell where the model is defined.\n",
        "L.plotLearningCurves()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arbIrFzDRZ71"
      },
      "source": [
        "We see a maximal accuracy of about 71% to less then 72%. There is not much, if any, improvement in the accuracy after about the first 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2XCut2kTAls"
      },
      "source": [
        "### Performance\n",
        "\n",
        "An accuracy of about 71%, which is already quite good for such a small network (and lot's of important things we did not even consider yet ...)!\n",
        "\n",
        "Let's check how well the model generalized by evaluating the validation data and the train data and manually computing the accuracy.\n",
        "* First, the model is put in *evaluation mode*. We *detach* then the data from the gradient tracking and move the data back to the CPU memory. Here, we can map to a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWXoT46HTAls"
      },
      "outputs": [],
      "source": [
        "# evaluate all training and validation data again for further study\n",
        "model.eval()\n",
        "predictions_train = model(c_tensor_train).detach().cpu().numpy()\n",
        "predictions_valid = model(c_tensor_valid).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfnlpzoxgIw7"
      },
      "source": [
        "The preddiction is a float between between 0 and 1. We cut at 0.5 and count everything below as QCD and everything above as Top. The true labels had been loaded above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsSlccZzfFLl"
      },
      "outputs": [],
      "source": [
        "# sklearn provides an accurcy_score for numpy arrays\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc_train = accuracy_score(predictions_train>0.5, labels_train)\n",
        "acc_valid = accuracy_score(predictions_valid>0.5, labels_valid)\n",
        "print(f\"Accuracy on traing data {acc_train}\")\n",
        "print(f\"Accuracy on validation data {acc_valid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpPCX6awTAls"
      },
      "source": [
        "This looks fairly similar, so for now, we don't seem to experience overtraining.\n",
        "\n",
        "We proceed by taking a look at the output distributions of the validation dataset, separated into signal and background components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0fwXCiGTAls"
      },
      "outputs": [],
      "source": [
        "hlp.plots.plot_hist(\n",
        "    (predictions_valid[labels_valid == 0], predictions_valid[labels_valid == 1]),\n",
        "    names=(\"QCD jets\", \"Top jets\"),\n",
        "    xlabel=\"Output distribution\",\n",
        "    log=True,\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm5oLLK0TAls"
      },
      "source": [
        "Besides the classification accuracy, we can study the *receiver operating characteristic* curve or **ROC** curve. It shows the relation between the true positive (jets *correctly* identified as top jets) and false positive rates (light jets *mistaken* as a top jets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYqJuYx9TAls"
      },
      "outputs": [],
      "source": [
        "# do the roc plot\n",
        "hlp.plots.plot_roc(\n",
        "    (labels_train, labels_valid),\n",
        "    (predictions_train, predictions_valid),\n",
        "    names=(\"train\", \"valid\"),\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBQboGwSTAls"
      },
      "source": [
        "The curves above are produced by scanning potential values to cut on the network output and examining the resulting signal classification (true positive) and background mis-classification (false positive) rates.\n",
        "\n",
        "Naturally, a well performing network has a high true positive rate while keeping the (reciprocal) false positive rate at a reasonably low (high) level. For the choice of the axes above, this would lead to a curve that is bent towards the upper right corner. But be aware that other representations of the ROC curve exist which might look somewhat different (e.g. \"1 - false positive rate\" on the y-axis). Their message is, however, identical.\n",
        "\n",
        "A commonly used proxy that compiles the values for all possible cuts into one metric is the area-under-curve - **AUC**. A value of 1 signalizes a perfectly working network that allows for a cut value leading to 100% signal efficiency and 0% background contamination. Opposed to that, a value of 0.5 means that the two output distributions of signal and background events are probably fully overlapping, lacking the opportunity to apply a cut that would favor signal examples. A value of 0 has the same logical meaning as 1, but the definition of what is signal and background is flipped. Therefore, the distance from 0.5 is what actually matters here.\n",
        "\n",
        "A value above 0.75 is already quite decent, but there's still potential. You can try to beat this value in the full training setup below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFC2x1sTAls"
      },
      "source": [
        "## Lessons learned\n",
        "\n",
        "- Now we know how to build a simple model using PyTorch.\n",
        "- To ensure model generalization, we evaluated validation data with our trainined model.\n",
        "- We calculated accuracies and visualized the output distributions.\n",
        "- We learned about ROC curves, AUC values and how to plot / compute them.\n",
        "\n",
        "With these tools at hand, we can jump into the next section and improve our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08sRysuMTAls"
      },
      "source": [
        "# Part 5: Improving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLevlrXMTAlu"
      },
      "source": [
        "Our input data - a selection of four-vectors with values given in GeV - clearly comes from the domain of physics. As we have seen in the plots above, their numerical values range from -500 to 500 for $p_x$ and $p_y$, and up to 2000 for $E$ and $p_z$ values.\n",
        "Restricting to a range of -1 to 1 is often improving the performance. sklearn provides several\n",
        "[scalers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#standardscaler). The MinMax scaler is popular but it depends strongly on the sample size. The StandardScaler is often a better choice. An alternative is to use a BatchNormalization layer as first layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UXHEq9ETAlu"
      },
      "source": [
        "There are many details that influence the performance of the model. An accuracy of 85% is easily possible.\n",
        "Do some experiment with several aspects of the model:\n",
        "* How many **epochs** are necessary to reach the best performance?\n",
        "* What is the best value for the **learning rate**.\n",
        "* **Preprocessing**\n",
        "  * Feature normalization: MinMax or StandardScaler\n",
        "  * Higher level features - Physical knowledge\n",
        "* Network **archtecture**\n",
        "  * how large can the layers become\n",
        "  * Activation functions - ReLU\n",
        "  * Convolutional layers (see separate lecture)\n",
        "* **Regularisation**\n",
        "  * Dropout\n",
        "  * Batchnorm is a now popular choice\n",
        "* Available training data size - **big data!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PeEP17WcYZ5"
      },
      "source": [
        "A possible sequence of experiments\n",
        "* _Tanh_ are especially sensitive to the proper normalization of the input features\n",
        "  * Try to use either the sklearn _MinMaxScaler_,  _StandardScaler_ or a _BatchNorm1d_ as first layer\n",
        "\n",
        "  * One _BatchNorm1d_ layer within the network is useful. It is not always obvious where to place it Try different position: 1st 2nd layer before after the activation\n",
        "  * An accuracy of about 78% should be possible\n",
        "  \n",
        "* Replace *Tanh* by *ReLU*. The resulting network typically performes better but it is also more sensitive to overtraining\n",
        "  * A first _BatchNorm1d_ is not useful\n",
        "  * A smaller learning rate in Adam should be beneficial (0.01-_0.005_-0.0001) but not too small\n",
        "  * A LR cool down is often beneficial to get the last little improvement. Uncomment the line where the optimizer (Adam) LR is modilfie with LR=0.001\n",
        "  * An accuracy of about 86-87% should be possible\n",
        "* Use the physically motivated transformation into invariants. Note that the input dimension is different\n",
        "  * We see an imediant increase in performance and faster traing. 30 epochs are typical enough\n",
        "  * An accuracy of about 90.2% should be possible\n",
        "* A larger datset for traing (all 8 files) should give a slightly better accuracy (90.4%)\n",
        "\n",
        "* The best model is not the last model. Save the best model during training. Load it and evaluate the model on the test dataset: Accuracy and ROC curve.\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCssdWd4Ael0"
      },
      "source": [
        "# Skorch\n",
        "Before we start with the experimenting, let's introduce [skorch](https://skorch.readthedocs.io/en/latest/user/quickstart.html).\n",
        "Pure PyTorch can be somewhat involved. There are tools to make our life easier by encapsulating the techincal details you have seen above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kITNFOuED-WS"
      },
      "source": [
        "Skorch works with callbacks. Callbacks are functions that are called at certain points in the processing loop. Especially: epoch start, epoch end, batch start, and batch end. Most common callbacks, e.g. for scoring, are predefined. The names explains the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMjtB-o-e_2_"
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import EpochScoring\n",
        "auc = EpochScoring(scoring='roc_auc',  on_train=False,lower_is_better=False)\n",
        "acc = EpochScoring(scoring='accuracy', on_train=True, lower_is_better=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVE19bAaE7uI"
      },
      "source": [
        "Also saving the best model (checkpoints) is automated. The [Checkpoint](https://skorch.readthedocs.io/en/latest/callbacks.html#skorch.callbacks.Checkpoint) callback saves by default the model with the lowest validation loss. The *load_best* flag the best modelAll loging is done in the directory *chkpoints*. (BTW The left panel in Colab allows you to look into your home dir.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keW21NVuEwrA"
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import Checkpoint\n",
        "cp = Checkpoint(dirname='chkpoints_1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzdQiLBCy719"
      },
      "source": [
        "First we repeat the preprocessing from above. In addition, we wrap the validation data into a skorch dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3m8K5S4e_g6"
      },
      "outputs": [],
      "source": [
        "from skorch.dataset import Dataset\n",
        "from skorch.helper import predefined_split\n",
        "\n",
        "def preprocess_constituents(constituents):\n",
        "    return constituents[:, :120].reshape((-1, 480)).astype(np.float32)\n",
        "\n",
        "# later we try something more fancy\n",
        "#def preprocess_constituents(constituents):\n",
        "#     c_sum=constituents.sum(axis=1)\n",
        "#     metric=np.array([1.,-1.,-1.,-1.])\n",
        "#     c_inv=(constituents*metric*c_sum[:,None,:]).sum(axis=2)\n",
        "#     return c_inv.astype('float32')\n",
        "\n",
        "# load training data\n",
        "c_vectors_train, _, labels_train = hlp.data.load(\"train\", stop_file=2)\n",
        "# the validation data\n",
        "c_vectors_valid, _, labels_valid = hlp.data.load(\"valid\", stop_file=1)\n",
        "\n",
        "# run the preprocessing\n",
        "c_vectors_train = preprocess_constituents(c_vectors_train)\n",
        "c_vectors_valid = preprocess_constituents(c_vectors_valid)\n",
        "\n",
        "#from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "#scaler = StandardScaler()\n",
        "##scaler = MinMaxScaler()  # worst idea\n",
        "#scaler.fit(c_vectors_train)\n",
        "#c_vectors_train=scaler.transform(c_vectors_train)\n",
        "#c_vectors_valid=scaler.transform(c_vectors_valid)\n",
        "\n",
        "ds_valid = Dataset(c_vectors_valid, labels_valid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FNycny_Gcld"
      },
      "source": [
        "Skorch provides a predefined training loop for a classifier. We put in\n",
        "* our model,\n",
        "* the loss as above (nn.BCELoss),\n",
        "* the Adam optimizer as above.\n",
        "* We use the same learning rate lr=0.005\n",
        "* and the same batch size of 500.\n",
        "* In addition, we add the data for validation.\n",
        "\n",
        "Skorch works with numpy and creates the torch datasets internally. Moving back and forth between GPU, if available, and CPU memory is also handle internally. The training loop is performed by just calling *fit()*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5h3v9TtDy1Y"
      },
      "source": [
        "We start with the same model but with the first experiment with adding a batch norm layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SboBlg6iDwnL"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.BatchNorm1d(480),\n",
        "    nn.Linear(480, 256, bias=True),\n",
        "    nn.Tanh(),\n",
        "    #nn.BatchNorm1d(256),\n",
        "    nn.Linear(256, 128, bias=True),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128,   1, bias=True),\n",
        "    nn.Sigmoid(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2gvBY1Zwl9S"
      },
      "outputs": [],
      "source": [
        "from skorch import NeuralNetClassifier\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    model,\n",
        "    criterion=nn.BCELoss(),\n",
        "    optimizer=torch.optim.Adam,\n",
        "    lr=0.005,\n",
        "    batch_size=500,\n",
        "    max_epochs=20,\n",
        "    callbacks=[acc,auc,cp],\n",
        "    train_split=predefined_split(ds_valid),\n",
        "    # Shuffle training data on each epoch\n",
        "    iterator_train__shuffle=True,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6xuLeW97p7h"
      },
      "outputs": [],
      "source": [
        "net.fit(c_vectors_train, labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJkkCTp0Vs-j"
      },
      "outputs": [],
      "source": [
        "# Skorch comes with a history object. We have predefined some plots.\n",
        "hlp.plots.plotLearningCurvesSkorch(net.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabPRoHa7NpF"
      },
      "source": [
        "Maybe not yet the optimal accuracy? Skorch allows to continue the traing: *partial_fit* instead of *fit*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Xb5-TK620f"
      },
      "outputs": [],
      "source": [
        "net.partial_fit(c_vectors_train, labels_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pnnkksv4uHd"
      },
      "source": [
        "The epoch with the best validation loss is marked by a '+'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn99qkMQ3VhZ"
      },
      "outputs": [],
      "source": [
        "hlp.plots.plotLearningCurvesSkorch(net.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCfwGQl4_lB"
      },
      "source": [
        "0.7288     0.7860        0.5173       0.7206        0.5295  \n",
        "The trained net can be used for predictions at the best (validation loss) model:\n",
        "```python\n",
        "net.load_params(checkpoint=cp)\n",
        "net.predict_proba(data)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDa2nBC8BSVx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "# net.predict_proba gives 2 probability (background/signal)\n",
        "# as expected from sklearn classifiers\n",
        "# here, we need only the second value\n",
        "net.load_params(checkpoint=cp)\n",
        "predictions_train = net.predict_proba(c_vectors_train)[:,1]\n",
        "predictions_valid = net.predict_proba(c_vectors_valid)[:,1]\n",
        "acc_train = accuracy_score(labels_train,predictions_train>0.5)\n",
        "auc_train = roc_auc_score(labels_train,predictions_train)\n",
        "acc_valid = accuracy_score(labels_valid,predictions_valid>0.5)\n",
        "auc_valid = roc_auc_score(labels_valid,predictions_valid)\n",
        "print(f'train      - acc {acc_train:.4f} \\tauc {auc_train:.4f} ')\n",
        "print(f'validation - acc {acc_valid:.4f} \\tauc {auc_valid:.4f} ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_30Hr3c3Zny"
      },
      "outputs": [],
      "source": [
        "# do the roc plot\n",
        "hlp.plots.plot_roc(\n",
        "    (labels_train, labels_valid),\n",
        "    (predictions_train, predictions_valid),\n",
        "    names=(\"train\", \"valid\"),\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WhCA3a87ndM"
      },
      "source": [
        "Happy experimenting!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "rise": {
      "controls": false,
      "footer": "&nbsp;&nbsp; <b>QU TensorFlow Tutorial</b> | 8. June 2021 | Marcel Rieger",
      "scroll": true,
      "slideNumber": "c",
      "theme": "simple",
      "transition": "none"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
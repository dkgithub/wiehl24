{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkgithub/wiehl24/blob/main/torch_tensor_tutorial_timing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7muouFbCq9so"
      },
      "source": [
        "### We check the speed differences between single/multicore CPU and GPU\n",
        "\n",
        "Make sure that you have a GPU available.\n",
        "**Runtime->Change Runtime-> T4 GPU**\n",
        "Typically, you get only one GPU for free. It will complain if you are running another collab notebooks with GPU.\n",
        "\n",
        "Jupyter (and Collab) notebooks have an build in timeing utillity:\n",
        "```python\n",
        "%timit command\n",
        "\n",
        "%%timeit\n",
        "complete notebook cell\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mkl-service"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3rPuYQW9R5F",
        "outputId": "15e0ece4-2765-429f-d25e-37a49678d952"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mkl-service\n",
            "  Downloading mkl_service-2.4.0-35-cp310-cp310-manylinux2014_x86_64.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from mkl-service) (2023.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mkl-service) (1.16.0)\n",
            "Requirement already satisfied: intel-openmp==2023.* in /usr/local/lib/python3.10/dist-packages (from mkl->mkl-service) (2023.2.3)\n",
            "Requirement already satisfied: tbb==2021.* in /usr/local/lib/python3.10/dist-packages (from mkl->mkl-service) (2021.11.0)\n",
            "Installing collected packages: mkl-service\n",
            "Successfully installed mkl-service-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "dO8e1hVPq9sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7cd96e-9229-408d-d5db-d3209d73de51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device index: 1. Total number of devices: 1\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "if not torch.cuda.is_available():\n",
        "    print('This tutorial expect a GPU')\n",
        "else:\n",
        "    torch.cuda.set_device(0)\n",
        "    num_gpus    = torch.cuda.device_count()\n",
        "    current_gpu = torch.cuda.device_count()\n",
        "    print(\"Current device index: {}. Total number of devices: {}\".format(current_gpu,num_gpus))\n",
        "    print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOOvVhP9q9sq"
      },
      "source": [
        "### Synchronize\n",
        "CUDA operation are performed asynchronous, i.e. the python code may continue before the operation is finished. By torch.cuda.synchronize() the program flow is forced to wait for completion. If you measure speed this is important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2o52BxVq9sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c7ce8b-85f9-4ae7-df31-82a233159b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.6 s ± 2.05 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "279 ms ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "# simple jupyter %timeit works but it may choose a too large number of loops\n",
        "# timeit adapts the number of tests(loops) to the speed of the operation.\n",
        "# Without torch.cuda.synchronize() the operation looks quicker then it really is\n",
        "# This example may then run a few minutes\n",
        "size=8192\n",
        "A=torch.randn(size,size)\n",
        "B=torch.randn(size,size)\n",
        "%timeit A.mm(B)\n",
        "\n",
        "A=A.cuda()\n",
        "B=B.cuda()\n",
        "%timeit A.mm(B);torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwmWrTrVq9sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61413dcb-a657-4457-b8b1-c52a4d939e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "279 ms ± 5.79 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# timing of cell behaves similar\n",
        "A.mm(B)\n",
        "torch.cuda.synchronize()\n",
        "# w/o torch.cuda.synchronize(): 1000 loops, best of 3: 121 ms per loop\n",
        "# with torch.cuda.synchronize():   1 loop, best of 3: 121 ms per loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7sYE1LTq9sr"
      },
      "source": [
        "## If yo run this on a multicore machine you can benefit from multi-threading MKL\n",
        "\n",
        "E.g. on a machine with 2 Intel Xeon E5‐2600v4, i.e. 20 cores,\n",
        "\n",
        "* numpy\n",
        "\n",
        " * 1 threads</br>\n",
        " Dotted two 4096x4096 matrices in 3.45 s.</br>\n",
        " Dotted two vectors of length 524288 in 0.23 ms.</br>\n",
        " SVD of a 2048x1024 matrix in 1.49 s.</br>\n",
        " Eigendecomposition of a 2048x2048 matrix in 8.95 s.</br>\n",
        "\n",
        " * 40 threads</br>\n",
        " Dotted two 4096x4096 matrices in 0.29 s.</br>\n",
        " Dotted two vectors of length 524288 in 0.03 ms.</br>\n",
        " SVD of a 2048x1024 matrix in 0.42 s.</br>\n",
        " Eigendecomposition of a 2048x2048 matrix in 3.93 s.</br>\n",
        "\n",
        "* torch - cpu\n",
        " * 1 thread</br>\n",
        " Dotted two 4096x4096 matrices in 1.74 s.</br>\n",
        " Dotted two vectors of length 524288 in 0.12 ms.</br>\n",
        " SVD of a 2048x1024 matrix in 2.81 s.</br>\n",
        " Eigendecomposition of a 2048x2048 matrix in 2.15 s.</br>\n",
        "\n",
        " * 40 threads</br>\n",
        " Dotted two 4096x4096 matrices in 0.13 s.</br>\n",
        " Dotted two vectors of length 524288 in 0.01 ms.</br>\n",
        " SVD of a 2048x1024 matrix in 0.81 s.</br>\n",
        " Eigendecomposition of a 2048x2048 matrix in 1.49 s.</br>\n",
        "\n",
        "* torch - cuda float32\n",
        " * Tesla P100-PCIE-16GB</br>\n",
        " Dotted two 4096x4096 matrices in 0.02 s.</br>\n",
        " Dotted two vectors of length 524288 in 0.05 ms.</br>\n",
        " SVD of a 2048x1024 matrix in 0.32 s.</br>\n",
        " Eigendecomposition of a 2048x2048 matrix in 1.40 s.</br></br>\n",
        " Dotted two 8192x8192 matrices in 0.12 s.</br>\n",
        " Dotted two vectors of length 1048576 in 0.06 ms.</br>\n",
        " SVD of a 4096x2048 matrix in 1.40 s.</br>\n",
        " Eigendecomposition of a 4096x4096 matrix in 6.86 s.</br>\n",
        " * torch - cuda float64</br>\n",
        " Dotted two 8192x8192 matrices in 0.24 s.</br>\n",
        " Dotted two vectors of length 1048576 in 0.07 ms.</br>\n",
        " SVD of a 4096x2048 matrix in 2.36 s.</br>\n",
        " Eigendecomposition of a 4096x4096 matrix in 12.61 s.</br>\n",
        "\n",
        "* numpy on google T4 only one CPU\n",
        "\n",
        " * 1 thread</br>\n",
        " Dotted two 4096x4096 matrices in 3.79 s. </br>\n",
        " Dotted two vectors of length 524288 in 0.38 ms.</br>\n",
        " SVD of a 2048x1024 matrix in 1.73 s.</br>\n",
        " Cholesky decomposition of a 2048x2048 matrix in 0.22 s </br>\n",
        " Eigendecomposition of a 2048x2048 matrix in 10.60 s.</br>\n",
        "\n",
        "* torch - cpu\n",
        "  * 1 thread  </br>\n",
        " Dotted two 4096x4096 matrices in 1.94 s. </br>\n",
        "Dotted two vectors of length 524288 in 0.17 ms. </br>\n",
        "SVD of a 2048x1024 matrix in 0.61 s. </br>\n",
        "Eigendecomposition of a 2048x2048 matrix in 3.60 s. </br>\n",
        "\n",
        "* torch - cuda float32\n",
        "  * Tesla T4</br>\n",
        "  Dotted two 4096x4096 matrices in 0.04 s.</br>\n",
        "  Dotted two vectors of length 524288 in 0.02 ms.</br>\n",
        "  SVD of a 2048x1024 matrix in 0.23 s.</br>\n",
        "  Eigendecomposition of a 2048x2048 matrix in 3.48 s.</br>\n",
        "\n",
        "\n",
        "Get the same numbers for your Colab machine\n",
        "Check Runtime - change to T4 GPU if available\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lS_B7-nGq9sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82ab9ab-3e75-454e-ee33-e7c9d46037f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dotted two 4096x4096 matrices in 3.79 s.\n",
            "Dotted two vectors of length 524288 in 0.38 ms.\n",
            "SVD of a 2048x1024 matrix in 1.73 s.\n",
            "Cholesky decomposition of a 2048x2048 matrix in 0.22 s.\n",
            "Eigendecomposition of a 2048x2048 matrix in 10.60 s.\n"
          ]
        }
      ],
      "source": [
        "# basic linear algebra operation with numpy\n",
        "import mkl\n",
        "mkl.set_num_threads(1)\n",
        "#mkl.set_num_threads(40)  # run this if you have more cores available for yourself\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Let's take the randomness out of random numbers (for reproducibility)\n",
        "np.random.seed(0)\n",
        "\n",
        "size = 4096\n",
        "A, B = np.random.random((size, size)), np.random.random((size, size))\n",
        "C, D = np.random.random((size * 128,)), np.random.random((size * 128,))\n",
        "E = np.random.random((int(size / 2), int(size / 4)))\n",
        "F = np.random.random((int(size / 2), int(size / 2)))\n",
        "F = np.dot(F, F.T)\n",
        "G = np.random.random((int(size / 2), int(size / 2)))\n",
        "\n",
        "# Matrix multiplication\n",
        "N = 20\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    np.dot(A, B)\n",
        "delta = time() - t\n",
        "print('Dotted two %dx%d matrices in %0.2f s.' % (size, size, delta / N))\n",
        "del A, B\n",
        "\n",
        "# Vector multiplication\n",
        "N = 5000\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    np.dot(C, D)\n",
        "delta = time() - t\n",
        "print('Dotted two vectors of length %d in %0.2f ms.' % (size * 128, 1e3 * delta / N))\n",
        "del C, D\n",
        "\n",
        "# Singular Value Decomposition (SVD)\n",
        "N = 3\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    np.linalg.svd(E, full_matrices = True)\n",
        "delta = time() - t\n",
        "print(\"SVD of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 4, delta / N))\n",
        "del E\n",
        "\n",
        "# Cholesky Decomposition\n",
        "N = 3\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    np.linalg.cholesky(F)\n",
        "delta = time() - t\n",
        "print(\"Cholesky decomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N))\n",
        "\n",
        "# Eigendecomposition\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    np.linalg.eig(G)\n",
        "delta = time() - t\n",
        "print(\"Eigendecomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a7Aj5vOqq9sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb14f96-7978-46f2-f633-5225e4049ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dotted two 4096x4096 matrices in 1.94 s.\n",
            "Dotted two vectors of length 524288 in 0.17 ms.\n",
            "SVD of a 2048x1024 matrix in 0.61 s.\n",
            "Eigendecomposition of a 2048x2048 matrix in 3.60 s.\n"
          ]
        }
      ],
      "source": [
        "# basic linear algebra operation with pytorch CPU\n",
        "import mkl\n",
        "mkl.set_num_threads(1)\n",
        "#mkl.set_num_threads(40)\n",
        "import torch\n",
        "from time import time\n",
        "\n",
        "# Let's take the randomness out of random numbers (for reproducibility)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "size = 4096\n",
        "A, B = torch.rand((size, size)), torch.rand((size, size))\n",
        "C, D = torch.rand((size * 128,)), torch.rand((size * 128,))\n",
        "E = torch.rand((int(size / 2), int(size / 4)))\n",
        "F = torch.rand((int(size / 2), int(size / 2)))\n",
        "F = torch.mm(F, F.t())\n",
        "G = torch.rand((int(size / 2), int(size / 2)))\n",
        "\n",
        "# Matrix multiplication\n",
        "N = 20\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.mm(A, B)\n",
        "delta = time() - t\n",
        "print('Dotted two %dx%d matrices in %0.2f s.' % (size, size, delta / N))\n",
        "del A, B\n",
        "\n",
        "# Vector multiplication\n",
        "N = 5000\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.dot(C, D)\n",
        "delta = time() - t\n",
        "print( 'Dotted two vectors of length %d in %0.2f ms.' % (size * 128, 1e3 * delta / N))\n",
        "del C, D\n",
        "\n",
        "# Singular Value Decomposition (SVD)\n",
        "N = 3\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.svd(E)\n",
        "delta = time() - t\n",
        "print( \"SVD of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 4, delta / N))\n",
        "del E\n",
        "\n",
        "\n",
        "# Eigendecomposition\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.linalg.eig(G)\n",
        "delta = time() - t\n",
        "print(\"Eigendecomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dI306nqpq9sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93debc3d-73d6-471e-ae7b-6f213f78f744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n",
            "True True\n",
            "Dotted two 4096x4096 matrices in 0.04 s.\n",
            "Dotted two vectors of length 524288 in 0.02 ms.\n",
            "SVD of a 2048x1024 matrix in 0.23 s.\n",
            "Eigendecomposition of a 2048x2048 matrix in 3.48 s.\n"
          ]
        }
      ],
      "source": [
        "# basic linear algebra operation with pytorch CUDA\n",
        "import mkl\n",
        "mkl.set_num_threads(1)\n",
        "import torch\n",
        "from time import time\n",
        "\n",
        "# Let's take the randomness out of random numbers (for reproducibility)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "dtype=torch.float32\n",
        "#dtype=torch.float64\n",
        "#size = 8192\n",
        "size = 4096\n",
        "\n",
        "A, B = torch.rand((size, size),dtype=dtype), torch.rand((size, size),dtype=dtype)\n",
        "C, D = torch.rand((size * 128,),dtype=dtype), torch.rand((size * 128,),dtype=dtype)\n",
        "E = torch.rand((int(size / 2), int(size / 4)),dtype=dtype)\n",
        "F = torch.rand((int(size / 2), int(size / 2)),dtype=dtype)\n",
        "F = torch.mm(F, F.t())\n",
        "G = torch.rand((int(size / 2), int(size / 2)),dtype=dtype)\n",
        "\n",
        "torch.cuda.device(0)\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "A=A.cuda()\n",
        "B=B.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print( B.is_cuda, A.is_cuda)\n",
        "\n",
        "# Matrix multiplication\n",
        "N = 20\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.mm(A, B)\n",
        "    # cuda operations are asynchronous - synchronize() waits that operation is finished\n",
        "    torch.cuda.synchronize()\n",
        "delta = time() - t\n",
        "print('Dotted two %dx%d matrices in %0.2f s.' % (size, size, delta / N))\n",
        "del A, B\n",
        "\n",
        "C=C.cuda()\n",
        "D=D.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "# Vector multiplication\n",
        "N = 5000\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.dot(C, D)\n",
        "    torch.cuda.synchronize()\n",
        "delta = time() - t\n",
        "print('Dotted two vectors of length %d in %0.2f ms.' % (size * 128, 1e3 * delta / N))\n",
        "del C, D\n",
        "\n",
        "E=E.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "# Singular Value Decomposition (SVD)\n",
        "N = 3\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.svd(E)\n",
        "    torch.cuda.synchronize()\n",
        "delta = time() - t\n",
        "print(\"SVD of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 4, delta / N))\n",
        "del E\n",
        "\n",
        "G=G.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Eigendecomposition\n",
        "t = time()\n",
        "for i in range(N):\n",
        "    torch.linalg.eig(G)\n",
        "    torch.cuda.synchronize()\n",
        "delta = time() - t\n",
        "print(\"Eigendecomposition of a %dx%d matrix in %0.2f s.\" % (size / 2, size / 2, delta / N))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enjoy the faster computations"
      ],
      "metadata": {
        "id": "r41m4JyA_o7_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6el1lvEJ_riT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}